#version: '3.8'

services:
  ollama:
    platform: linux/arm64
    build: .
    container_name: ollama-service
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./models:/models
      - ./Modelfile:/Modelfile
    networks:
      - ollama_network
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 12G
    environment:
      - CUDA_VISIBLE_DEVICES=''
      - METAL_DEVICE_WRITABLE=1
    restart: unless-stopped
    healthcheck:
      test: curl --fail http://localhost:11434/api/tags || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  streamlit-interface:
    platform: linux/arm64
    build:
      context: .
      dockerfile: Dockerfile.streamlit
    ports:
      - "8501:8501"
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - ollama_network
    restart: unless-stopped
    volumes:
      - ./knowledge_base.json:/app/knowledge_base.json

  document-analyzer:
    platform: linux/arm64
    build:
      context: .
      dockerfile: Dockerfile.analyzer
    ports:
      - "8502:8501"
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ./uploads:/app/uploads
      - ./knowledge_base.json:/app/knowledge_base.json
    networks:
      - ollama_network
    restart: unless-stopped

  tech-support:
    platform: linux/arm64
    build:
      context: .
      dockerfile: Dockerfile.streamlit
    ports:
      - "8503:8501"
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ./tech_support.py:/app/tech_support.py
    networks:
      - ollama_network
    restart: unless-stopped
    command: [ "streamlit", "run", "tech_support.py", "--server.address", "0.0.0.0" ]

volumes:
  ollama_data:


networks:
  ollama_network:
    name: ollama_network
